import nltk
 from nltk.corpus import stopwords
 from nltk.tokenize import word_tokenize
 from nltk.stem import PorterStemmer, WordNetLemmatizer
 from collections import Counter
 
 
 nltk.download('punkt')
 nltk.download('punkt_tab')
 nltk.download('stopwords')
 nltk.download('wordnet')
 
 corpus = ["My name is Sourabh Singh Bais. I am a CSE student."]
 
 def tokenize(text):
     return word_tokenize(text.lower())
 
 def remove_stopwords(tokens):
     stop_words = set(stopwords.words('english'))
     return [word for word in tokens if word not in stop_words]def stemming_and_lemmatization(tokens):
     ps = PorterStemmer()
     lemmatizer = WordNetLemmatizer()
     stemmed_tokens = [ps.stem(word) for word in tokens]
     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
     return stemmed_tokens, lemmatized_tokens
 
 def token_frequency(tokens):
     return Counter(tokens)
 
 def get_bigrams(tokens):
     bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
     return bigrams
 
 result = []
 for i in corpus:
     tokens = tokenize(i)
     without_stopwords = remove_stopwords(tokens)
     result.extend(without_stopwords)
 
 stemmed_tokens, lemmatized_tokens = stemming_and_lemmatization(result)
 
 frequency = token_frequency(result)
 bigrams = get_bigrams(result)
 print("Tokens after stopword removal:",result )
 print("Stemmed Tokens:", stemmed_tokens)
 
 print("Lemmatized Tokens:", lemmatized_tokens)
 print("Token Frequency:", frequency)
 print("Bigrams:", bigrams)
